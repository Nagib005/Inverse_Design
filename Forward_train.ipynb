{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdSUbnJ0t4V3W1/8lFyfqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagib005/Inverse_Design/blob/main/Forward_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYlhxQX5T71g",
        "outputId": "d25ff246-53a6-41d7-dce7-b327bf6f91fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adadelta\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Specify the path to your CSV file in Drive\n",
        "file_path = '/content/drive/MyDrive/Circular_inverse/result_V.csv'  # <-- Change this to your actual path\n",
        "\n",
        "# Step 3: Load the data from CSV file (results of FDTD solver)\n",
        "result = pd.read_csv(file_path, header=None)\n",
        "result = result.to_numpy()\n",
        "\n",
        "x = result[:, 0:6]\n",
        "y = result[:, 6:8]\n",
        "\n",
        "# Allocation of 70% of the total data to the training data\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.30, shuffle=True)\n",
        "# Allocation of 50% of the remaining data to the validation data and 50% to the test data (15% validation and 15% test of total)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_val, y_val, test_size=0.50, shuffle=True)\n",
        "\n",
        "# Feature Scaling\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_val = sc.transform(x_val)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# Free up RAM\n",
        "del result\n",
        "\n",
        "# Define the Layers of the deep neural network (DNN)\n",
        "Model = Sequential()\n",
        "Model.add(Dense(60, input_dim=6))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "Model.add(Dense(60))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "Model.add(Dense(60))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "Model.add(Dense(60))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "Model.add(Dense(60))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "Model.add(Dense(60))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dense(2))\n",
        "Model.summary()\n",
        "\n",
        "# EarlyStopping callback\n",
        "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=2)\n",
        "\n",
        "# Compile model\n",
        "Model.compile(loss='mean_squared_logarithmic_error',\n",
        "              optimizer=Adadelta(learning_rate=0.1))\n",
        "\n",
        "# Train model\n",
        "history = Model.fit(x_train, y_train, epochs=5000,\n",
        "                    batch_size=80, validation_data=(x_val, y_val), callbacks=[es])\n",
        "\n",
        "# Plot loss graph\n",
        "plt.plot(history.history['val_loss'], linewidth=1)\n",
        "plt.plot(history.history['loss'], linewidth=2, linestyle='--')\n",
        "plt.title('The loss of training model', fontname='Times New Roman', fontsize=18, loc='center')\n",
        "plt.xlabel('epochs', fontname='Times New Roman', fontsize=18)\n",
        "plt.ylabel('loss', fontname='Times New Roman', fontsize=18)\n",
        "plt.xticks(fontfamily='Times New Roman', fontsize=14)\n",
        "plt.yticks(fontfamily='Times New Roman', fontsize=14)\n",
        "plt.legend(['Validation Loss', 'Training Loss'])\n",
        "plt.show()\n",
        "\n",
        "# Loss values of train and validation data\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Test model\n",
        "predictions = Model.predict(x_test)\n",
        "Loss = Model.evaluate(x_test, y_test)\n",
        "print('Test loss:', Loss)\n",
        "\n",
        "# Save loss values to pickle and CSV files\n",
        "with open('history_Forward_model.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "fieldnames = ['Epoch', 'Training Loss', 'Validation Loss']\n",
        "with open('loss_Forward_model.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for epoch, train_loss_value, val_loss_value in zip(range(1, len(train_loss) + 1), train_loss, val_loss):\n",
        "        writer.writerow({'Epoch': epoch, 'Training Loss': train_loss_value, 'Validation Loss': val_loss_value})\n",
        "\n",
        "# Save model architecture and weights\n",
        "model_json = Model.to_json()\n",
        "with open(\"T-shaped_switch_Nozhat_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "Model.save_weights(\"T-shaped_switch_Nozhat_model_weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adadelta\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Path to your CSV file in Drive â€” update this\n",
        "file_path = '/content/drive/MyDrive/Circular_inverse/result_V.csv'\n",
        "\n",
        "# Step 3: Define chunk size and dtype for memory efficiency\n",
        "chunksize = 30000\n",
        "dtypes = {i: 'float32' for i in range(8)}  # Assuming 8 columns total\n",
        "\n",
        "# Step 4: Read CSV in chunks and collect data\n",
        "x_chunks = []\n",
        "y_chunks = []\n",
        "\n",
        "for chunk in pd.read_csv(file_path, header=None, chunksize=chunksize, dtype=dtypes):\n",
        "    x_chunks.append(chunk.iloc[:, 0:6].values)\n",
        "    y_chunks.append(chunk.iloc[:, 6:8].values)\n",
        "\n",
        "# Step 5: Stack chunks vertically into full arrays\n",
        "x = np.vstack(x_chunks)\n",
        "y = np.vstack(y_chunks)\n",
        "\n",
        "# Step 6: Split data into training, validation, and test sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.30, shuffle=True)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_val, y_val, test_size=0.50, shuffle=True)\n",
        "\n",
        "# Step 7: Feature scaling\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_val = sc.transform(x_val)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# Clear memory of raw chunks\n",
        "del x_chunks, y_chunks, x, y\n",
        "\n",
        "# Step 8: Define the model architecture\n",
        "Model = Sequential()\n",
        "Model.add(Dense(60, input_dim=6))\n",
        "Model.add(LeakyReLU(alpha=0.2))\n",
        "Model.add(Dropout(0.1))\n",
        "for _ in range(5):\n",
        "    Model.add(Dense(60))\n",
        "    Model.add(LeakyReLU(alpha=0.2))\n",
        "    Model.add(Dropout(0.1))\n",
        "Model.add(Dense(2))\n",
        "Model.summary()\n",
        "\n",
        "# Step 9: Compile the model\n",
        "Model.compile(loss='mean_squared_logarithmic_error', optimizer=Adadelta(learning_rate=0.1))\n",
        "\n",
        "# Step 10: Early stopping callback\n",
        "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=2)\n",
        "\n",
        "# Step 11: Train the model\n",
        "history = Model.fit(x_train, y_train, epochs=5000, batch_size=80,\n",
        "                    validation_data=(x_val, y_val), callbacks=[es])\n",
        "\n",
        "# Step 12: Plot training and validation loss\n",
        "plt.plot(history.history['val_loss'], linewidth=1)\n",
        "plt.plot(history.history['loss'], linewidth=2, linestyle='--')\n",
        "plt.title('Training Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Validation Loss', 'Training Loss'])\n",
        "plt.show()\n",
        "\n",
        "# Step 13: Evaluate on test set\n",
        "loss = Model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss}')\n",
        "\n",
        "# Step 14: Save training history and model\n",
        "with open('history_Forward_model.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "fieldnames = ['Epoch', 'Training Loss', 'Validation Loss']\n",
        "with open('loss_Forward_model.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for epoch, train_loss_value, val_loss_value in zip(range(1, len(history.history['loss']) + 1),\n",
        "                                                      history.history['loss'], history.history['val_loss']):\n",
        "        writer.writerow({'Epoch': epoch, 'Training Loss': train_loss_value, 'Validation Loss': val_loss_value})\n",
        "\n",
        "model_json = Model.to_json()\n",
        "with open(\"T-shaped_switch_Nozhat_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "Model.save_weights(\"T-shaped_switch_Nozhat_model_weights.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGALvd37Ylu7",
        "outputId": "ecc3d282-23fa-4e49-b996-414bc6b77e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU, Dropout\n",
        "from keras.optimizers import Adadelta\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Circular_inverse/result_V.csv'  # <-- Update this!\n",
        "\n",
        "chunksize = 30000\n",
        "dtypes = {i: 'float32' for i in range(8)}  # assuming 8 columns total\n",
        "\n",
        "# Step 1: Initialize scaler and fit incrementally (partial fit)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(\"Fitting scaler incrementally on chunks...\")\n",
        "for chunk in pd.read_csv(file_path, header=None, chunksize=chunksize, dtype=dtypes):\n",
        "    x_chunk = chunk.iloc[:, 0:6].values\n",
        "    scaler.partial_fit(x_chunk)\n",
        "\n",
        "# Step 2: Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(60, input_dim=6))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(Dropout(0.1))\n",
        "for _ in range(5):\n",
        "    model.add(Dense(60))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.1))\n",
        "model.add(Dense(2))\n",
        "\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer=Adadelta(learning_rate=0.1))\n",
        "\n",
        "# Step 3: Incremental training loop\n",
        "epochs = 10  # Number of full passes over data\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for chunk in pd.read_csv(file_path, header=None, chunksize=chunksize, dtype=dtypes):\n",
        "        x_chunk = chunk.iloc[:, 0:6].values\n",
        "        y_chunk = chunk.iloc[:, 6:8].values\n",
        "        x_chunk = scaler.transform(x_chunk)\n",
        "        # Train on current chunk\n",
        "        loss = model.train_on_batch(x_chunk, y_chunk)\n",
        "    print(f\"Last chunk training loss: {loss:.5f}\")\n",
        "\n",
        "# Step 4 (Optional): Save your trained model\n",
        "model.save_weights('/content/drive/MyDrive/Circular_inverse/T-shaped_switch_Nozhat_model_weights.h5')\n",
        "model_json = model.to_json()\n",
        "with open(\"T-shaped_switch_Nozhat_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"Training complete and model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y7Tyh_21bC6W",
        "outputId": "32b5f50d-5af9-4657-92e5-aa9600eb9351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fitting scaler incrementally on chunks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Last chunk training loss: 0.00571\n",
            "\n",
            "Epoch 2/10\n",
            "Last chunk training loss: 0.00459\n",
            "\n",
            "Epoch 3/10\n",
            "Last chunk training loss: 0.00418\n",
            "\n",
            "Epoch 4/10\n",
            "Last chunk training loss: 0.00396\n",
            "\n",
            "Epoch 5/10\n",
            "Last chunk training loss: 0.00382\n",
            "\n",
            "Epoch 6/10\n",
            "Last chunk training loss: 0.00372\n",
            "\n",
            "Epoch 7/10\n",
            "Last chunk training loss: 0.00365\n",
            "\n",
            "Epoch 8/10\n",
            "Last chunk training loss: 0.00360\n",
            "\n",
            "Epoch 9/10\n",
            "Last chunk training loss: 0.00356\n",
            "\n",
            "Epoch 10/10\n",
            "Last chunk training loss: 0.00352\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The filename must end in `.weights.h5`. Received: filepath=T-shaped_switch_Nozhat_model_weights.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-97396217d837>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Step 4 (Optional): Save your trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'T-shaped_switch_Nozhat_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T-shaped_switch_Nozhat_model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;34m\"The filename must end in `.weights.h5`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;34mf\"Received: filepath={filepath}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=T-shaped_switch_Nozhat_model_weights.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 (Optional): Save your trained model\n",
        "model.save_weights('/content/drive/MyDrive/Circular_inverse/T-shaped_switch_Nozhat_model.weights.h5')\n",
        "model_json = model.to_json()\n",
        "with open(\"T-shaped_switch_Nozhat_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"Training complete and model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdXB4LeH-a0L",
        "outputId": "ae28656a-b7c6-4080-a193-3d1ccf26eec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete and model saved.\n"
          ]
        }
      ]
    }
  ]
}